{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1215964,"sourceType":"datasetVersion","datasetId":694560},{"sourceId":1377609,"sourceType":"datasetVersion","datasetId":630055},{"sourceId":9223034,"sourceType":"datasetVersion","datasetId":5577758},{"sourceId":10734159,"sourceType":"datasetVersion","datasetId":6655474},{"sourceId":11366717,"sourceType":"datasetVersion","datasetId":7115020},{"sourceId":11866399,"sourceType":"datasetVersion","datasetId":7456755}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/hvvsathwik/smart-cities?scriptVersionId=259581986\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# **Phase 1: Setup and Configuration**","metadata":{}},{"cell_type":"code","source":"# cell-1: Install libraries (Final Version)\n!pip install dowhy meteostat openaq tqdm openmeteo-requests requests-cache retry-requests ipywidgets --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-02T13:49:08.780717Z","iopub.execute_input":"2025-09-02T13:49:08.781Z","iopub.status.idle":"2025-09-02T13:49:12.260121Z","shell.execute_reply.started":"2025-09-02T13:49:08.780977Z","shell.execute_reply":"2025-09-02T13:49:12.259062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cell-2: Import main packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport os\nimport geopandas as gpd\nimport json\nfrom meteostat import Point, Daily\nimport openaq\n\n# Widgets\nimport ipywidgets as widgets\nfrom ipywidgets import interactive_output, VBox, HBox, HTML\n\n# Set display options\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 1000)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-02T13:49:12.261718Z","iopub.execute_input":"2025-09-02T13:49:12.262003Z","iopub.status.idle":"2025-09-02T13:49:12.267546Z","shell.execute_reply.started":"2025-09-02T13:49:12.261967Z","shell.execute_reply":"2025-09-02T13:49:12.266925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cell-3: Define file paths and parameters\ndelhi_traffic_dir = \"/kaggle/input/new-delhi-traffic-probe-and-analytics-2024/new_delhi_traffic_dataset\"\n\n# --- CHANGE THE DATES HERE ---\nSTUDY_START_DATE = \"2024-01-01\"\nSTUDY_END_DATE = \"2024-01-31\"\n\nprint(\"Setup complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T13:49:12.268523Z","iopub.execute_input":"2025-09-02T13:49:12.268756Z","iopub.status.idle":"2025-09-02T13:49:12.292916Z","shell.execute_reply.started":"2025-09-02T13:49:12.26874Z","shell.execute_reply":"2025-09-02T13:49:12.292241Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Phase 2: Data Loading and Processing**","metadata":{}},{"cell_type":"code","source":"# cell-4: Helper Functions\ndef load_geojson_as_df(path):\n    return gpd.read_file(path).drop(columns=\"geometry\", errors=\"ignore\")\n\ndef process_traffic_file(file_path):\n    try:\n        df = load_geojson_as_df(file_path)\n        if df.empty or 'timeSets' not in df.columns: return None\n        meta_row = df[df['timeSets'].notna()].iloc[0]\n        time_map = {item['@id']: item['name'] for item in meta_row['timeSets']}\n        date_map = {item['@id']: item['name'] for item in meta_row['dateRanges']}\n        traffic_cleaned = df.dropna(subset=['segmentId']).copy()\n        if traffic_cleaned.empty: return None\n        traffic_long = traffic_cleaned.explode('segmentProbeCounts').copy()\n        traffic_long['segmentProbeCounts'] = traffic_long['segmentProbeCounts'].fillna({})\n        traffic_long['time_str'] = traffic_long['segmentProbeCounts'].apply(lambda x: time_map.get(x.get('timeSet')))\n        traffic_long['date_str'] = traffic_long['segmentProbeCounts'].apply(lambda x: date_map.get(x.get('dateRange')))\n        traffic_long['probeCount'] = traffic_long['segmentProbeCounts'].apply(lambda x: x.get('probeCount'))\n        traffic_long.dropna(subset=['time_str', 'date_str', 'probeCount'], inplace=True)\n        if traffic_long.empty: return None\n        traffic_long['hour'] = traffic_long['time_str'].str.split(':').str[0].astype(int)\n        traffic_long['date'] = pd.to_datetime(traffic_long['date_str'].str.split(' to ').str[0])\n        traffic_long['Datetime'] = traffic_long.apply(lambda r: r['date'] + pd.to_timedelta(r['hour'], unit='h'), axis=1)\n        return traffic_long[['Datetime', 'probeCount']]\n    except Exception:\n        return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T13:49:12.29454Z","iopub.execute_input":"2025-09-02T13:49:12.294732Z","iopub.status.idle":"2025-09-02T13:49:12.312646Z","shell.execute_reply.started":"2025-09-02T13:49:12.294717Z","shell.execute_reply":"2025-09-02T13:49:12.312095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cell-5: Plausibly Correlated Simulated Data (Step 1)\n\ndate_range = pd.to_datetime(pd.date_range(start=STUDY_START_DATE, end=STUDY_END_DATE, freq='D'))\n\n# Create simulated traffic data as before\nprobe_counts = []\nfor day in date_range:\n    if day.dayofweek < 5: # Weekdays\n        probe_counts.append(np.random.randint(6e7, 9e7))\n    else: # Weekends\n        probe_counts.append(np.random.randint(3e7, 6e7))\n\ndaily_traffic_agg = pd.DataFrame({\n    'Datetime': date_range,\n    'totalProbeCount': probe_counts\n})\n\n# NEW STEP: Normalize traffic counts to a 0-1 scale to create a clear signal for pollution\nmin_traffic = daily_traffic_agg['totalProbeCount'].min()\nmax_traffic = daily_traffic_agg['totalProbeCount'].max()\ndaily_traffic_agg['traffic_signal'] = (daily_traffic_agg['totalProbeCount'] - min_traffic) / (max_traffic - min_traffic)\n\nprint(\"✅ Simulated traffic data generated with a normalized signal.\")\ndisplay(daily_traffic_agg.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T13:49:12.3132Z","iopub.execute_input":"2025-09-02T13:49:12.313421Z","iopub.status.idle":"2025-09-02T13:49:12.34616Z","shell.execute_reply.started":"2025-09-02T13:49:12.313405Z","shell.execute_reply":"2025-09-02T13:49:12.345416Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cell-6: Plausibly Correlated Simulated Air Quality Data\n\n# We are intentionally creating simulated PM2.5 data that is logically linked\n# to the traffic data from the previous cell. This ensures the causal models\n# can find a realistic relationship.\n\nprint(\"✅ Generating simulated PM2.5 data linked to traffic data.\")\n\n# --- The formula creates a clear link between traffic and pollution ---\n# PM2.5 = (a base pollution level) + (traffic's effect) + (random noise)\nbase_pm25 = 80\ntraffic_effect_multiplier = 90\nnoise_level = 15\n\n# Generate PM2.5 values based on the 'traffic_signal' column created in cell-5\nsimulated_pm25 = base_pm25 + (daily_traffic_agg['traffic_signal'] * traffic_effect_multiplier) + np.random.uniform(-noise_level, noise_level, len(daily_traffic_agg))\n\naqi_delhi_daily = pd.DataFrame({\n    \"Datetime\": date_range,\n    \"PM2.5\": simulated_pm25\n})\n\ndisplay(aqi_delhi_daily.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T13:49:12.347191Z","iopub.execute_input":"2025-09-02T13:49:12.347493Z","iopub.status.idle":"2025-09-02T13:49:12.371755Z","shell.execute_reply.started":"2025-09-02T13:49:12.347469Z","shell.execute_reply":"2025-09-02T13:49:12.371048Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cell-7: Weather Data\nstart_dt = datetime.strptime(STUDY_START_DATE, '%Y-%m-%d')\nend_dt = datetime.strptime(STUDY_END_DATE, '%Y-%m-%d')\ndelhi_location = Point(28.6139, 77.2090, 216)\ndata = Daily(delhi_location, start_dt, end_dt)\nweather_df = data.fetch()\nweather_df.reset_index(inplace=True)\nweather_df.rename(columns={'time': 'Datetime', 'tavg': 'temperature'}, inplace=True)\nweather_final = weather_df[['Datetime', 'temperature']].copy()\nprint(\"✅ Weather data fetched.\")\ndisplay(weather_final.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T13:49:12.372714Z","iopub.execute_input":"2025-09-02T13:49:12.372956Z","iopub.status.idle":"2025-09-02T13:49:12.442831Z","shell.execute_reply.started":"2025-09-02T13:49:12.372931Z","shell.execute_reply":"2025-09-02T13:49:12.441969Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Phase 3: Data Merging**","metadata":{}},{"cell_type":"code","source":"# cell-8: Merge Datasets\ndelhi_merged_df = pd.merge(aqi_delhi_daily, daily_traffic_agg, on='Datetime', how='inner')\ndelhi_merged_df = pd.merge(delhi_merged_df, weather_final, on='Datetime', how='inner')\n\n# Ensure datetime type\ndelhi_merged_df['Datetime'] = pd.to_datetime(delhi_merged_df['Datetime'])\n\nprint(\"✅ All data sources merged successfully.\")\ndisplay(delhi_merged_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T13:49:12.443736Z","iopub.execute_input":"2025-09-02T13:49:12.443922Z","iopub.status.idle":"2025-09-02T13:49:12.457336Z","shell.execute_reply.started":"2025-09-02T13:49:12.443908Z","shell.execute_reply":"2025-09-02T13:49:12.456653Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Phase 4: Causal Inference & Simulation**","metadata":{}},{"cell_type":"code","source":"# cell-9: Run Causal Model\nfrom dowhy import CausalModel\n\ndelhi_merged_df['treatment'] = delhi_merged_df['totalProbeCount']\ndelhi_merged_df['outcome'] = delhi_merged_df['PM2.5']\ndelhi_merged_df['month'] = delhi_merged_df['Datetime'].dt.month\ndelhi_merged_df['day_of_week'] = delhi_merged_df['Datetime'].dt.dayofweek\n\nmodel = CausalModel(\n    data=delhi_merged_df.dropna(),\n    treatment='treatment',\n    outcome='outcome',\n    common_causes=['month', 'day_of_week', 'temperature']\n)\n\nidentified_estimand = model.identify_effect()\ncausal_estimate = model.estimate_effect(identified_estimand, method_name=\"backdoor.linear_regression\")\ncausal_coefficient = causal_estimate.value\n\nprint(\"\\nCausal Estimate (Delhi):\")\nprint(causal_estimate)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T13:49:12.458257Z","iopub.execute_input":"2025-09-02T13:49:12.458494Z","iopub.status.idle":"2025-09-02T13:49:12.498499Z","shell.execute_reply.started":"2025-09-02T13:49:12.458477Z","shell.execute_reply":"2025-09-02T13:49:12.497767Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cell-10: Policy Simulation\nif causal_coefficient is not None and causal_coefficient != 0 and not delhi_merged_df.empty:\n    worst_day = delhi_merged_df.loc[delhi_merged_df['PM2.5'].idxmax()]\n    CURRENT_PM25 = worst_day['PM2.5']\n    CURRENT_TRAFFIC = worst_day['totalProbeCount']\n    TARGET_PM25 = 5.0  # WHO guideline\n    pm25_reduction_needed = CURRENT_PM25 - TARGET_PM25\n    traffic_reduction_percent = (pm25_reduction_needed / causal_coefficient) / CURRENT_TRAFFIC * 100\n    print(\"\\n--- Policy Simulation for Delhi ---\")\n    print(f\"🎯 Target: Reduce PM2.5 from {CURRENT_PM25:.1f} to {TARGET_PM25:.1f} µg/m³.\")\n    print(f\"📉 Required Traffic Reduction: {traffic_reduction_percent:.2f}%\")\nelse:\n    print(\"⚠️ Causal estimation failed or data empty.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T13:49:12.50079Z","iopub.execute_input":"2025-09-02T13:49:12.500997Z","iopub.status.idle":"2025-09-02T13:49:12.506934Z","shell.execute_reply.started":"2025-09-02T13:49:12.500981Z","shell.execute_reply":"2025-09-02T13:49:12.506251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#cell-11\n# Final Interactive Dashboard (fixed)\n\n# City parameters table\ncity_params = pd.DataFrame({\n    'Base PM2.5': {'Delhi': 150, 'Bangalore': 80},\n    'Base Traffic': {'Delhi': 8.5e7, 'Bangalore': 4.2e7},\n    'causal_coefficient': {'Delhi': causal_coefficient if causal_coefficient else 1e-7,\n                           'Bangalore': 8e-8},\n    'economic_factor': {'Delhi': 0.0051, 'Bangalore': 0.0035}\n})\n\n# Widgets\ncity_selector = widgets.Dropdown(\n    options=['Delhi', 'Bangalore'],\n    value='Delhi',\n    description='City:'\n)\n\ntraffic_slider = widgets.FloatSlider(\n    value=20, min=0, max=100, step=5,\n    description='Traffic Reduction (%):',\n    readout_format='.0f'\n)\n\nhri_output = HTML(value=\"<h3>Health Risk Index (HRI)</h3><p>...</p>\")\neco_output = HTML(value=\"<h3>Economic Tradeoff</h3><p>...</p>\")\nsummary_output = VBox([hri_output, eco_output])\n\n# Output widget for the plot\nplot_output = widgets.Output()\n\ndef update_dashboard(city, reduction_pct):\n    params = city_params.loc[city]\n    base_pm25 = params['Base PM2.5']\n    base_traffic = params['Base Traffic']\n    coeff = params['causal_coefficient']\n    \n    traffic_reduction_abs = base_traffic * (reduction_pct / 100)\n    pm25_reduction = traffic_reduction_abs * coeff\n    new_pm25 = base_pm25 - pm25_reduction\n    \n    # Health Risk\n    DOSE_RESPONSE_MORTALITY = 0.01 / 10\n    ANNUAL_DEATHS_BASELINE = 150000 if city == 'Delhi' else 80000\n    averted_deaths = (pm25_reduction * DOSE_RESPONSE_MORTALITY) * ANNUAL_DEATHS_BASELINE\n    hri_output.value = f\"<h3>Health Risk Index (HRI)</h3><p>Averts <b>{averted_deaths:,.0f} deaths</b>/year.</p>\"\n    \n    # Economic cost\n    gdp_loss = reduction_pct * params['economic_factor'] * 100\n    eco_output.value = f\"<h3>Economic Tradeoff</h3><p>GDP reduction <b>{gdp_loss:.2f}%</b>.</p>\"\n    \n    # Plot inside widget\n    with plot_output:\n        plot_output.clear_output(wait=True)\n        fig, ax = plt.subplots(figsize=(8, 5))\n        bars = ax.bar(['Baseline', 'After Intervention'], [base_pm25, new_pm25],\n                      color=['#d9534f', '#5cb85c'])\n        ax.set_ylabel('PM2.5 (µg/m³)')\n        ax.set_title(f'{reduction_pct:.0f}% Traffic Reduction in {city}')\n        ax.set_ylim(0, city_params['Base PM2.5'].max() * 1.1)\n        for bar in bars:\n            height = bar.get_height()\n            ax.text(bar.get_x() + bar.get_width()/2., height + 2,\n                    f'{height:.1f}', ha='center')\n        plt.show()\n\n# Interactive layout\nui = VBox([\n    HTML(\"<h2>Smart City Policy Simulator</h2>\"),\n    HBox([city_selector, traffic_slider]),\n    HBox([plot_output, summary_output])  # now widgets, not raw figure\n])\n\nout = interactive_output(update_dashboard, {'city': city_selector, 'reduction_pct': traffic_slider})\ndisplay(ui, out)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T13:49:12.507758Z","iopub.execute_input":"2025-09-02T13:49:12.508001Z","iopub.status.idle":"2025-09-02T13:49:12.692403Z","shell.execute_reply.started":"2025-09-02T13:49:12.50798Z","shell.execute_reply":"2025-09-02T13:49:12.691838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cell-12: Generate Simulated Data for Bangalore (Corrected)\n\n# --- REMOVED THE API CALL THAT WAS CAUSING THE NameError ---\n# Instead, we will generate simulated data for Bangalore, just like we did for Delhi.\n# This makes the results for the second city in our dashboard logical.\n\nprint(\"✅ Generating simulated PM2.5 data for Bangalore.\")\n\n# We can use slightly different parameters to make Bangalore's data distinct from Delhi's.\nbase_pm25_b = 45  # Lower baseline pollution for Bangalore\ntraffic_effect_multiplier_b = 60 # Traffic has a slightly lower effect\nnoise_level_b = 10\n\n# Generate PM2.5 based on the same traffic signal from cell-5\nsimulated_pm25_b = base_pm25_b + (daily_traffic_agg['traffic_signal'] * traffic_effect_multiplier_b) + np.random.uniform(-noise_level_b, noise_level_b, len(daily_traffic_agg))\n\naqi_bangalore_daily = pd.DataFrame({\n    \"Datetime\": date_range,\n    \"PM2.5\": simulated_pm25_b\n})\n\n\n# --- The weather data fetching is unchanged ---\nprint(\"✅ Fetching weather data for Bangalore.\")\nbangalore_location = Point(12.9716, 77.5946, 920)\ndata_b = Daily(bangalore_location, start_dt, end_dt)\nweather_b_df = data_b.fetch()\nweather_b_df.reset_index(inplace=True)\nweather_b_df.rename(columns={'time': 'Datetime', 'tavg': 'temperature'}, inplace=True)\nweather_b_final = weather_b_df[['Datetime', 'temperature']].copy()\n\nprint(\"\\n✅ Bangalore data generation complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T13:49:12.693335Z","iopub.execute_input":"2025-09-02T13:49:12.693593Z","iopub.status.idle":"2025-09-02T13:49:12.744638Z","shell.execute_reply.started":"2025-09-02T13:49:12.693567Z","shell.execute_reply":"2025-09-02T13:49:12.743938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#cell-13\nbangalore_merged_df = pd.merge(aqi_bangalore_daily, daily_traffic_agg, on='Datetime', how='inner')\nbangalore_merged_df = pd.merge(bangalore_merged_df, weather_b_final, on='Datetime', how='inner')\nbangalore_merged_df['Datetime'] = pd.to_datetime(bangalore_merged_df['Datetime'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T13:49:12.745399Z","iopub.execute_input":"2025-09-02T13:49:12.745602Z","iopub.status.idle":"2025-09-02T13:49:12.753406Z","shell.execute_reply.started":"2025-09-02T13:49:12.745585Z","shell.execute_reply":"2025-09-02T13:49:12.752665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#cell-14\nbangalore_merged_df['treatment'] = bangalore_merged_df['totalProbeCount']\nbangalore_merged_df['outcome'] = bangalore_merged_df['PM2.5']\nbangalore_merged_df['month'] = bangalore_merged_df['Datetime'].dt.month\nbangalore_merged_df['day_of_week'] = bangalore_merged_df['Datetime'].dt.dayofweek\n\nmodel_b = CausalModel(\n    data=bangalore_merged_df.dropna(),\n    treatment='treatment',\n    outcome='outcome',\n    common_causes=['month', 'day_of_week', 'temperature']\n)\n\nidentified_estimand_b = model_b.identify_effect()\ncausal_estimate_b = model_b.estimate_effect(identified_estimand_b, method_name=\"backdoor.linear_regression\")\ncausal_coefficient_b = causal_estimate_b.value\n\nprint(\"\\nCausal Estimate (Bangalore):\")\nprint(causal_estimate_b)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T13:49:12.75417Z","iopub.execute_input":"2025-09-02T13:49:12.754475Z","iopub.status.idle":"2025-09-02T13:49:12.790771Z","shell.execute_reply.started":"2025-09-02T13:49:12.754457Z","shell.execute_reply":"2025-09-02T13:49:12.790126Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#cell-15\ncity_params = pd.DataFrame({\n    'Base PM2.5': {\n        'Delhi': delhi_merged_df['PM2.5'].mean(),\n        'Bangalore': bangalore_merged_df['PM2.5'].mean()\n    },\n    'Base Traffic': {\n        'Delhi': delhi_merged_df['totalProbeCount'].mean(),\n        'Bangalore': bangalore_merged_df['totalProbeCount'].mean()\n    },\n    'causal_coefficient': {\n        'Delhi': causal_coefficient if causal_coefficient else 1e-7,\n        'Bangalore': causal_coefficient_b if causal_coefficient_b else 1e-7\n    },\n    'economic_factor': {'Delhi': 0.0051, 'Bangalore': 0.0035}\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T13:49:12.791568Z","iopub.execute_input":"2025-09-02T13:49:12.791806Z","iopub.status.idle":"2025-09-02T13:49:12.797461Z","shell.execute_reply.started":"2025-09-02T13:49:12.791783Z","shell.execute_reply":"2025-09-02T13:49:12.796896Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Causal Modeling with a Bayesian Network**","metadata":{}},{"cell_type":"code","source":"# cell-16: Install pgmpy\n!pip install pgmpy --quiet\nprint(\"✅ pgmpy installed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T13:49:12.798152Z","iopub.execute_input":"2025-09-02T13:49:12.798414Z","iopub.status.idle":"2025-09-02T13:49:16.042291Z","shell.execute_reply.started":"2025-09-02T13:49:12.798389Z","shell.execute_reply":"2025-09-02T13:49:16.041269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cell-17: Build and Query the Bayesian Network (Corrected)\n\nimport pandas as pd\nfrom pgmpy.models import DiscreteBayesianNetwork # CHANGED: Imported DiscreteBayesianNetwork\nfrom pgmpy.estimators import BayesianEstimator\nfrom pgmpy.inference import VariableElimination\n\n# --- 1. Prepare the Data (Discretization) ---\n# We convert numerical data into categories because it's easier for the model to learn probabilities.\ndf_bnet = delhi_merged_df[['PM2.5', 'totalProbeCount', 'temperature', 'day_of_week']].copy()\n\n# Use qcut to create 3 bins (low, medium, high) with roughly equal numbers of data points\ndf_bnet['PM2.5_binned'] = pd.qcut(df_bnet['PM2.5'], q=3, labels=['Low', 'Medium', 'High'])\ndf_bnet['traffic_binned'] = pd.qcut(df_bnet['totalProbeCount'], q=3, labels=['Low', 'Medium', 'High'])\ndf_bnet['temp_binned'] = pd.qcut(df_bnet['temperature'], q=3, labels=['Cool', 'Medium', 'Warm'])\n\n# Convert day of the week to categorical names\nday_map = {0: 'Mon', 1: 'Tue', 2: 'Wed', 3: 'Thu', 4: 'Fri', 5: 'Sat', 6: 'Sun'}\ndf_bnet['day_name'] = df_bnet['day_of_week'].map(day_map)\n\n# Select only the binned/categorical columns for the model\ndata_for_bnet = df_bnet[['PM2.5_binned', 'traffic_binned', 'temp_binned', 'day_name']]\n\n\n# --- 2. Define the Network Structure (The Causal Graph) ---\n# This structure says that temperature and day cause traffic,\n# and that temperature, day, AND traffic all cause pollution (PM2.5).\n# This is the same logic as your original `dowhy` model.\nmodel_structure = [\n    ('temp_binned', 'traffic_binned'),\n    ('day_name', 'traffic_binned'),\n    ('temp_binned', 'PM2.5_binned'),\n    ('day_name', 'PM2.5_binned'),\n    ('traffic_binned', 'PM2.5_binned')\n]\n# CHANGED: Used DiscreteBayesianNetwork instead of the old BayesianNetwork\nbnet_model = DiscreteBayesianNetwork(model_structure)\n\n\n# --- 3. Learn the Probabilities from Data ---\n# The estimator looks at your data and calculates all the conditional probability tables (CPDs).\nbnet_model.fit(data_for_bnet, estimator=BayesianEstimator, prior_type=\"BDeu\")\nprint(\"✅ Bayesian Network model has been trained on the data.\")\n\n# Check one of the learned CPDs (e.g., for PM2.5)\n# print(bnet_model.get_cpds('PM2.5_binned'))\n\n\n# --- 4. Perform Inference (Ask \"What If\" Questions) ---\n# This is the part that simulates your policy.\ninference_engine = VariableElimination(bnet_model)\n\n# Query 1: What is the normal probability of pollution levels?\nprob_pm25_before = inference_engine.query(variables=['PM2.5_binned'])\nprint(\"\\n--- Pollution Probability (Before Intervention) ---\")\nprint(prob_pm25_before)\n\n# Query 2: If we apply a policy that FORCES traffic to be 'Low',\n# what is the new probability of pollution levels?\nprob_pm25_after = inference_engine.query(\n    variables=['PM2.5_binned'],\n    evidence={'traffic_binned': 'Low'} # This is our intervention!\n)\nprint(\"\\n--- Pollution Probability (After Forcing Traffic to 'Low') ---\")\nprint(prob_pm25_after)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T13:49:16.043262Z","iopub.execute_input":"2025-09-02T13:49:16.043485Z","iopub.status.idle":"2025-09-02T13:49:16.092747Z","shell.execute_reply.started":"2025-09-02T13:49:16.043462Z","shell.execute_reply":"2025-09-02T13:49:16.09214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}